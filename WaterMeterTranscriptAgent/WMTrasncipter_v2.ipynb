{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "yOSEa8OgSirU",
        "outputId": "00de1aa3-e528-40e5-b0fb-51f72ddd6989"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain_core in c:\\users\\vidad\\documents\\wizzzzy\\ai-agents\\.venv\\lib\\site-packages (0.3.68)\n",
            "Requirement already satisfied: langgraph in c:\\users\\vidad\\documents\\wizzzzy\\ai-agents\\.venv\\lib\\site-packages (0.5.3)\n",
            "Requirement already satisfied: python-dotenv in c:\\users\\vidad\\documents\\wizzzzy\\ai-agents\\.venv\\lib\\site-packages (1.1.1)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in c:\\users\\vidad\\documents\\wizzzzy\\ai-agents\\.venv\\lib\\site-packages (from langchain_core) (0.4.5)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\vidad\\documents\\wizzzzy\\ai-agents\\.venv\\lib\\site-packages (from langchain_core) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\vidad\\documents\\wizzzzy\\ai-agents\\.venv\\lib\\site-packages (from langchain_core) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\vidad\\documents\\wizzzzy\\ai-agents\\.venv\\lib\\site-packages (from langchain_core) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\vidad\\documents\\wizzzzy\\ai-agents\\.venv\\lib\\site-packages (from langchain_core) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\vidad\\documents\\wizzzzy\\ai-agents\\.venv\\lib\\site-packages (from langchain_core) (4.14.1)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in c:\\users\\vidad\\documents\\wizzzzy\\ai-agents\\.venv\\lib\\site-packages (from langchain_core) (2.11.7)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\vidad\\documents\\wizzzzy\\ai-agents\\.venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain_core) (3.0.0)\n",
            "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.1.0 in c:\\users\\vidad\\documents\\wizzzzy\\ai-agents\\.venv\\lib\\site-packages (from langgraph) (2.1.0)\n",
            "Requirement already satisfied: langgraph-prebuilt<0.6.0,>=0.5.0 in c:\\users\\vidad\\documents\\wizzzzy\\ai-agents\\.venv\\lib\\site-packages (from langgraph) (0.5.2)\n",
            "Requirement already satisfied: langgraph-sdk<0.2.0,>=0.1.42 in c:\\users\\vidad\\documents\\wizzzzy\\ai-agents\\.venv\\lib\\site-packages (from langgraph) (0.1.73)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in c:\\users\\vidad\\documents\\wizzzzy\\ai-agents\\.venv\\lib\\site-packages (from langgraph) (3.5.0)\n",
            "Requirement already satisfied: ormsgpack>=1.10.0 in c:\\users\\vidad\\documents\\wizzzzy\\ai-agents\\.venv\\lib\\site-packages (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph) (1.10.0)\n",
            "Requirement already satisfied: httpx>=0.25.2 in c:\\users\\vidad\\documents\\wizzzzy\\ai-agents\\.venv\\lib\\site-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in c:\\users\\vidad\\documents\\wizzzzy\\ai-agents\\.venv\\lib\\site-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10.18)\n",
            "Requirement already satisfied: anyio in c:\\users\\vidad\\documents\\wizzzzy\\ai-agents\\.venv\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (4.9.0)\n",
            "Requirement already satisfied: certifi in c:\\users\\vidad\\documents\\wizzzzy\\ai-agents\\.venv\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\users\\vidad\\documents\\wizzzzy\\ai-agents\\.venv\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.0.9)\n",
            "Requirement already satisfied: idna in c:\\users\\vidad\\documents\\wizzzzy\\ai-agents\\.venv\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in c:\\users\\vidad\\documents\\wizzzzy\\ai-agents\\.venv\\lib\\site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.16.0)\n",
            "Requirement already satisfied: requests<3,>=2 in c:\\users\\vidad\\documents\\wizzzzy\\ai-agents\\.venv\\lib\\site-packages (from langsmith>=0.3.45->langchain_core) (2.32.4)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\vidad\\documents\\wizzzzy\\ai-agents\\.venv\\lib\\site-packages (from langsmith>=0.3.45->langchain_core) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\vidad\\documents\\wizzzzy\\ai-agents\\.venv\\lib\\site-packages (from langsmith>=0.3.45->langchain_core) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\vidad\\documents\\wizzzzy\\ai-agents\\.venv\\lib\\site-packages (from pydantic>=2.7.4->langchain_core) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\vidad\\documents\\wizzzzy\\ai-agents\\.venv\\lib\\site-packages (from pydantic>=2.7.4->langchain_core) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\vidad\\documents\\wizzzzy\\ai-agents\\.venv\\lib\\site-packages (from pydantic>=2.7.4->langchain_core) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\vidad\\documents\\wizzzzy\\ai-agents\\.venv\\lib\\site-packages (from requests<3,>=2->langsmith>=0.3.45->langchain_core) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\vidad\\documents\\wizzzzy\\ai-agents\\.venv\\lib\\site-packages (from requests<3,>=2->langsmith>=0.3.45->langchain_core) (2.5.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in c:\\users\\vidad\\documents\\wizzzzy\\ai-agents\\.venv\\lib\\site-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.3.1)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install langchain_core langgraph python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "mv4KCxwyTDd_"
      },
      "outputs": [],
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from langchain_core.tools import BaseTool\n",
        "from typing import Optional, Type, Dict, List, Any, Tuple\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage\n",
        "from typing import TypedDict, Annotated, Sequence\n",
        "from langgraph.graph.message import add_messages\n",
        "from IPython.display import display, Image\n",
        "\n",
        "\n",
        "import json\n",
        "import re\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cixMd1PJTQxH"
      },
      "source": [
        "## **Initialize Pydantic Tools and Input**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "V1Ksr0jITL0t"
      },
      "outputs": [],
      "source": [
        "# Input model for the agent\n",
        "class AgentInput(BaseModel):\n",
        "    \"\"\"Input model for the agent\"\"\"\n",
        "    input_text: str\n",
        "\n",
        "class RegexExtractionTool(BaseTool):\n",
        "    \"\"\"Tool for extracting unit readings using regex\"\"\"\n",
        "    name: str = \"regex_extraction\"\n",
        "    description: str = \"Extract unit readings from natural language text using regex pattern\"\n",
        "    pattern: str = r'(?:Unit\\s+)?(\\d+[A-Z])\\s+(?:reads|is|reading)\\s+(\\d+(?:\\.\\d+)?)\\s++(?:cubic\\s+meter|m\\^3|m3)'\n",
        "\n",
        "    def _run(self, input_text: str) -> List[Tuple[str, str]]: # Ex. Output = [('Unit 1', '123.45'), ('Unit 2', '67.89')]\n",
        "        \"\"\"Extract unit and reading pairs from input text\"\"\"\n",
        "        matches = re.findall(self.pattern, input_text, re.IGNORECASE)\n",
        "        return matches\n",
        "\n",
        "    async def _arun(self, input_text: str) -> List[Tuple[str, str]]:\n",
        "        \"\"\"Async version of _run\"\"\"\n",
        "        return self._run(input_text)\n",
        "\n",
        "class ValidationTool(BaseTool):\n",
        "    \"\"\"Tool for validating extracted unit readings\"\"\"\n",
        "    name: str = \"validation\"\n",
        "    description: str = \"Validate extracted unit readings for duplicates and conflicts\"\n",
        "\n",
        "    def _run(self, matches: List[Tuple[str, str]]) -> Dict[str, Any]: # Ex. Output = {'is_valid': True, 'errors': [], 'duplicates_found': False, 'conflicts_found': False, 'conflicts': {}}\n",
        "        \"\"\"Validate matches and return validation results\"\"\"\n",
        "        validation_result = {\n",
        "            \"is_valid\": True,\n",
        "            \"errors\": [],\n",
        "            \"duplicates_found\": False,\n",
        "            \"conflicts_found\": False,\n",
        "            \"conflicts\": {}\n",
        "        }\n",
        "\n",
        "        # Check validity of matches\n",
        "        if not matches:\n",
        "            validation_result[\"is_valid\"] = False\n",
        "            validation_result[\"errors\"].append(\"No unit readings found\")\n",
        "            return validation_result\n",
        "\n",
        "        # Check for exact duplicates\n",
        "        seen_units = set()\n",
        "        for unit, reading in matches:\n",
        "            if unit in seen_units:\n",
        "                validation_result[\"duplicates_found\"] = True\n",
        "                validation_result[\"errors\"].append(f\"Duplicate unit found: {unit}\")\n",
        "            seen_units.add(unit)\n",
        "\n",
        "        # Check for conflicting values\n",
        "        unit_readings = defaultdict(list)\n",
        "        for unit, reading in matches:\n",
        "            unit_readings[unit].append(reading)\n",
        "\n",
        "        conflicts = {}\n",
        "        for unit, readings in unit_readings.items():\n",
        "            unique_readings = list(set(readings))\n",
        "            if len(unique_readings) > 1:\n",
        "                conflicts[unit] = unique_readings\n",
        "                validation_result[\"conflicts_found\"] = True\n",
        "\n",
        "        if conflicts:\n",
        "            validation_result[\"is_valid\"] = False\n",
        "            validation_result[\"conflicts\"] = conflicts\n",
        "            for unit, values in conflicts.items():\n",
        "                validation_result[\"errors\"].append(\n",
        "                    f\"Conflicting readings for Unit {unit}: {', '.join(values)} cubic meters\"\n",
        "                )\n",
        "\n",
        "        return validation_result\n",
        "\n",
        "    async def _arun(self, matches: List[Tuple[str, str]]) -> Dict[str, Any]:\n",
        "        \"\"\"Async version of _run\"\"\"\n",
        "        return self._run(matches)\n",
        "\n",
        "class RemoveDuplicates(BaseTool):\n",
        "    \"\"\"Tool for processing validated data into JSON format\"\"\"\n",
        "    name: str = \"remove_duplicates\"\n",
        "    description: str = \"Clean unit reading duplicates from input matches\"\n",
        "\n",
        "    def _run(self, matches: List[Tuple[str, str]]) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Process matches into structured data format\"\"\"\n",
        "        result = []\n",
        "\n",
        "        # Remove duplicates while preserving order\n",
        "        seen_units = set()\n",
        "        unique_matches = []\n",
        "        for unit, reading in matches:\n",
        "            if unit not in seen_units:\n",
        "                unique_matches.append((unit, reading))\n",
        "                seen_units.add(unit)\n",
        "    \n",
        "        return unique_matches\n",
        "\n",
        "class CreateJSON(BaseTool):\n",
        "    \"\"\"Convert validated matches into JSON file\"\"\"\n",
        "\n",
        "    name: str = \"data_processing\"\n",
        "    description: str = \"Process validated unit readings into structured JSON format\"\n",
        "\n",
        "    def _run(self, matches: List[Tuple[str, str]]) -> str: \n",
        "\n",
        "        result = []\n",
        "\n",
        "        for unit, reading in matches:\n",
        "            unit_data = {\n",
        "                \"unit\": unit,\n",
        "                \"reading\": int(float(reading))\n",
        "            }\n",
        "            result.append(unit_data)\n",
        "\n",
        "        json_output = json.dumps(result, indent=4)\n",
        "\n",
        "        return result\n",
        "\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "PSBYdS6gXziw",
        "outputId": "1124ac1a-c4e6-4cd2-b23e-e45b8dfcb6e9"
      },
      "outputs": [],
      "source": [
        "# Custom Reasoner class for unit reading processing\n",
        "class UnitReadingReasoner:\n",
        "    def __init__(self, tools: List[BaseTool]):\n",
        "        self.tools = {tool.name: tool for tool in tools}\n",
        "        self.processing_steps = [\n",
        "            \"regex_extraction\",\n",
        "            \"validation\",\n",
        "            \"remove_duplicates\",\n",
        "            \"data_processing\"\n",
        "        ]\n",
        "        # Initialize log for thoughts, actions, and observations\n",
        "        self.log = []\n",
        "        self.step = 0\n",
        "\n",
        "    def log_tao(self, type: str, description: str):\n",
        "        \"\"\"Log thought, action, and observation.\"\"\"\n",
        "        self.log.append(f\"{type}: {description}\")\n",
        "        print(f\"{type}: {description}\")\n",
        "\n",
        "    def _think_initial(self, user_input: str) -> str:\n",
        "            \"\"\"Initial reasoning about the input.\"\"\"\n",
        "            if self._contains_unit_readings(user_input):\n",
        "                return \"I can see unit readings in the input. I need to extract them first.\"\n",
        "            else:\n",
        "                return \"I don't see any unit readings in this input. I should inform the user.\"\n",
        "    \n",
        "    def reason(self, user_input: str) -> Tuple[str, List[Dict[str, Any]]]:\n",
        "\n",
        "        \"\"\"Main reasoning method that decides processing steps for unit readings.\"\"\"\n",
        "        # Check if input contains unit reading patterns\n",
        "        print(f\"{80 * '='}\\n ReAct Simulation\\n{80 * '='}\")\n",
        "    \n",
        "        self.log_tao(\"💭 Thought\", \"I need to check if there is a viable input\") \n",
        "\n",
        "        if self._contains_unit_readings(user_input):\n",
        "            tool_calls = [\n",
        "                {\n",
        "                    'tool': 'regex_extraction',\n",
        "                    'args': {'input_text': user_input},\n",
        "                    'step': 1\n",
        "                }\n",
        "            ]\n",
        "            self.log_tao(\"💭 Thought\", \"I'll process your unit readings step by step.\") \n",
        "            self.step = 1\n",
        "            return \"I'll process your unit readings step by step.\", tool_calls\n",
        "        else:\n",
        "            return \"I don't detect any unit readings in your input. Please provide text containing unit readings in the format 'Unit 1A reads 123.45 cubic meter'.\", []\n",
        "\n",
        "    def _contains_unit_readings(self, text: str) -> bool:\n",
        "        \"\"\"Check if text contains unit reading patterns.\"\"\"\n",
        "        # Log the action of checking for inputs\n",
        "        self.log_tao(\"🔧 Action\", \"Checking for inputs using _contains_unit_readings()\") \n",
        "\n",
        "        keywords = [\"cubic meter\", \"m3\", \"m^3\"]\n",
        "\n",
        "        if any(keyword in text for keyword in keywords):\n",
        "            # Log the observation of viable input\n",
        "            self.log_tao(\"👁️ Observation\", \"Viable input found\")\n",
        "            return True\n",
        "        \n",
        "        else:\n",
        "            self.log_tao(\"👁️ Observation\", \"No viable input found\")\n",
        "            return False\n",
        "        \n",
        "    def execute_tool_call(self, tool_name: str, args: Dict[str, Any]) -> Any:\n",
        "        \"\"\"Execute a tool call with given arguments.\"\"\"\n",
        "        if tool_name not in self.tools:\n",
        "            return f\"Error: Tool '{tool_name}' not found\"\n",
        "\n",
        "        tool = self.tools[tool_name]\n",
        "        try:\n",
        "            # Pass args directly to the tool's _run method\n",
        "            if hasattr(tool, '_run'):\n",
        "                self.log_tao(\"🔧 Action\", f\"Executing tool '{tool_name}' with args: {args}\")\n",
        "                # if self.step == 4:\n",
        "                #     self.log_tao(\"👁️ Observation\", \"Final JSON is ready to export\")\n",
        "                #     print(f\"{80 * '='}\\n\")\n",
        "                return tool._run(**args)\n",
        "            else:\n",
        "                return f\"Error: Tool '{tool_name}' does not have a _run method.\"\n",
        "        except Exception as e:\n",
        "            return f\"Error executing {tool_name}: {str(e)}\"\n",
        "\n",
        "    def get_next_step(self, current_step: int, validation_result: Dict[str, Any] = None) -> Optional[Dict[str, Any]]:\n",
        "        \"\"\"Determine the next processing step based on current results.\"\"\"\n",
        "        if current_step == 1:  # After regex extraction\n",
        "            self.log_tao(\"👁️ Observation\", \"Unit and Water Meter Reading pairs found\")\n",
        "            self.log_tao(\"💭 Thought\", \"I'll validate the extracted unit readings\")\n",
        "            self.step = 2\n",
        "\n",
        "            return {\n",
        "                'tool': 'validation',\n",
        "                'args': {'matches': None},  # Will be filled with extraction results\n",
        "                'step': 2\n",
        "            }\n",
        "        elif current_step == 2:  # After validation\n",
        "            if validation_result and validation_result.get('is_valid', False):\n",
        "                if validation_result.get('duplicates_found', False):\n",
        "                    errors = validation_result.get('errors', [])\n",
        "                    for error in errors:\n",
        "                        self.log_tao(\"👁️ Observation\", f\"Duplicates found, {error}\")\n",
        "                        self.log_tao(\"💭 Thought\", \"I'll remove extraction matches duplicates\")\n",
        "                        \n",
        "                    self.step = 3\n",
        "\n",
        "                    return {\n",
        "                    'tool': 'remove_duplicates',\n",
        "                    'args': {'matches': None},  # Will be filled with extraction results\n",
        "                    'step': 3\n",
        "                    }\n",
        "                \n",
        "                else:\n",
        "                    self.log_tao(\"👁️ Observation\", \"Validation done, no issues found\")\n",
        "                    self.log_tao(\"💭 Thought\", \"I'll process the validated matches into JSON file\")\n",
        "                    self.step = 4\n",
        "                    \n",
        "                    return {\n",
        "                    'tool': 'data_processing',\n",
        "                    'args': {'matches': None},  # Will be filled with extraction results\n",
        "                    'step': 4\n",
        "                    }\n",
        "                \n",
        "            else:\n",
        "                if validation_result.get('conflicts_found', False):\n",
        "                    conflicts = validation_result.get('conflicts', {})\n",
        "                    for unit, readings in conflicts.items():\n",
        "                        self.log_tao(\"👁️ Observation\", f\"Conflicting values found, Unit {unit}: {', '.join(readings)}\")\n",
        "            \n",
        "                print(f\"{80 * '='}\\n\")\n",
        "                return None  # Stop processing if validation failed\n",
        "            \n",
        "        \n",
        "        else:\n",
        "            self.log_tao(\"👁️ Observation\", \"Final JSON is ready to export\")\n",
        "        \n",
        "            print(f\"{80 * '='}\\n\")\n",
        "            return None\n",
        "    \n",
        "    def synthesize_results(self, user_input: str, all_results: List[Dict[str, Any]]) -> str:\n",
        "        \"\"\"Synthesize all processing results into a coherent response.\"\"\"\n",
        "        if not all_results:\n",
        "            return \"No results to process.\"\n",
        "\n",
        "        response_parts = []\n",
        "\n",
        "        # Find results by step\n",
        "        extraction_result = next((r for r in all_results if r.get('step') == 1), None)\n",
        "        validation_result = next((r for r in all_results if r.get('step') == 2), None)\n",
        "        duplicates_result = next((r for r in all_results if r.get('step') == 3), None)\n",
        "        processing_result = next((r for r in all_results if r.get('step') == 4), None)\n",
        "\n",
        "        if extraction_result:\n",
        "            matches = extraction_result.get('output')\n",
        "            response_parts.append(f\"📊 **Extraction Results:**\")\n",
        "            if matches:\n",
        "                response_parts.append(f\"Found {len(matches)} unit readings:\")\n",
        "                for unit, reading in matches:\n",
        "                    response_parts.append(f\"  - Unit {unit}: {reading} cubic meters\")\n",
        "            else:\n",
        "                response_parts.append(\"No unit readings found in the input.\")\n",
        "\n",
        "        if validation_result:\n",
        "            validation_data = validation_result.get('output')\n",
        "            response_parts.append(f\"\\n✅ **Validation Results:**\")\n",
        "\n",
        "            if validation_data and validation_data.get('is_valid', False):\n",
        "                response_parts.append(\"All unit readings are valid!\")\n",
        "            else:\n",
        "                response_parts.append(\"Validation issues found:\")\n",
        "                errors = validation_data.get('errors', []) if validation_data else []\n",
        "                for error in errors:\n",
        "                    response_parts.append(f\"  ⚠️ {error}\")\n",
        "\n",
        "            if validation_data and validation_data.get('duplicates_found', False):\n",
        "                response_parts.append(\"  🔄 Duplicates detected\")\n",
        "\n",
        "            if validation_data and validation_data.get('conflicts_found', False):\n",
        "                response_parts.append(\"  ⚡ Conflicts detected:\")\n",
        "                conflicts = validation_data.get('conflicts', {}) if validation_data else {}\n",
        "                for unit, readings in conflicts.items():\n",
        "                    response_parts.append(f\"    - Unit {unit}: {', '.join(readings)}\")\n",
        "\n",
        "        if duplicates_result:\n",
        "            unique_matches = duplicates_result.get('output')\n",
        "            response_parts.append(f\"\\n🔄 **Duplicates Removed:**\")\n",
        "            if unique_matches:\n",
        "                response_parts.append(f\"Found {len(unique_matches)} unique unit readings:\")\n",
        "                for unit, reading in unique_matches:\n",
        "                    response_parts.append(f\"  - Unit {unit}: {reading} cubic meters\")\n",
        "            else:\n",
        "                response_parts.append(\"No unique unit readings found after removing duplicates.\")\n",
        "        \n",
        "\n",
        "        if processing_result:\n",
        "            processed_data = processing_result.get('output')\n",
        "            response_parts.append(f\"\\n🔧 **Final Processed Data:**\")\n",
        "            if processed_data:\n",
        "                response_parts.append(\"```json\")\n",
        "                response_parts.append(json.dumps(processed_data, indent=2))\n",
        "                response_parts.append(\"```\")\n",
        "\n",
        "                response_parts.append(f\"\\n📈 **Summary:**\")\n",
        "                response_parts.append(f\"Successfully processed {len(processed_data)} unique unit readings\")\n",
        "            else:\n",
        "                response_parts.append(\"No data processed.\")\n",
        "\n",
        "\n",
        "        return \"\\n\".join(response_parts)\n",
        "    \n",
        "# Workflow State\n",
        "class WorkflowState(TypedDict):\n",
        "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
        "    current_step: int\n",
        "    extraction_results: List[Tuple[str, str]]\n",
        "    validation_results: Dict[str, Any]\n",
        "    processing_results: List[Dict[str, Any]]\n",
        "    pending_tool_calls: List[Dict[str, Any]]\n",
        "    all_results: List[Dict[str, Any]]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Initialize LangGraph Workflow**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "z4ZB_dh161BH"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAANgAAAD5CAIAAADKsmwpAAAAAXNSR0IArs4c6QAAIABJREFUeJztnXdcFNf+v89sb7QtdBAsiIiKATUSY8OYYETF3m4sv1y9liQkGu81ucbc5KvGG3M1otFg9EaJigXEHkUTQUEiqKAUQUFQelu2953fH+uLcHGp7uycZc/zyh+7O7Nz3hsez3zmzMwZDMdxgECQDYXsAAgEQCIiYAGJiIACJCICCpCICChAIiKggEZ2AOjQqg0NlVqlzKCU6Q16XKe1geEtJptCY2AcBxrHgeLmyyY7Tk/A0DiiCaVc//iuvDRP0VSjcXZlcByoHAeaI5+m09jA/x86iyKu0SplehoDKy9U9g3m9R3K7TeUR3auboBEBDiOZ5xvrClTiXxYfYO53gM4ZCd6JbRqY2me/HmRqvKJKjxKEPCaA9mJuoS9i1j4h/R6Ql14lOC1iS5kZ7EwMrEu43yjUqaf/Bd3riPsNZhdi5iWVE+lgzeiRGQHIZCmWk3y3qpJC918A6Hu6e1XxN9P1fHdGMPGOpMdxBqc3V/5+hSBmy+L7CDtYqcino+r8hnICRlnFxaaOLuvMnCE48AwSEtGexxHzDjf4NmPbVcWAgCmr/K695u4oUpDdhDz2J2Ij+/LAAChEb3t0KQrLNjgm5ZUjxth3AfanYipifXDJ9ijhSb6DuHdOttAdgoz2JeI92+IA8Mc2Twq2UFII2Sc8+P7coVUT3aQttiXiGX5itFRfLJTkMzYmcKc1GayU7TFjkQsK1DQ6BQq1Y5+sll8A7l56RKyU7TFjv4qTx8q/IdwrdzoP/7xj7Nnz/bgi2+99VZlZSUBiQCDRRF5MyufqIjYeI+xIxGb6rT9rC5iQUFBD75VXV0tFosJiPOCgOG8iidK4rbfA+xFRK3a2FCpYfOIOuWanp6+cuXKMWPGzJgxY/PmzQ0NDQCAsLCwqqqqr7/+evz48QAAuVy+f//+JUuWmFbbuXOnWq02fT0iIuL48eN//etfw8LCUlNTo6KiAADTp09ft24dEWm5TvT6CsgGFHH7oKlWE7+ljKCNFxYWhoaGHjhwoLq6Oj09ff78+WvWrMFxXK1Wh4aGJicnm1Y7cODAqFGjUlJSsrKyfvvtt8jIyO+//9606O23354zZ863336bmZmp0+lu3rwZGhpaUVFBUODaclXCd88I2njPgP2iDEuhkOi5TkT92JycHBaLtXz5cgqF4u7uHhQU9OTJk5dXW7x4cUREhL+/v+ltbm5uRkbGhx9+CADAMMzJyWn9+vUEJWwD14mmkMA1gmMvIhqNgMEmqg4JCQlRq9UxMTGjRo0aO3asj49PWFjYy6vR6fTbt29v3ry5uLhYr9cDAPj8P8eSgoKCCIr3MhQaxmDBVZXBlYY4uI5USb2OoI0HBgbu3r1bJBLFxsZGR0evXr06Nzf35dViY2Pj4uKio6OTk5Ozs7OXLVvWeimDwSAo3ssomvVUGma15rqCvYjIcaQpiTydEB4evmnTpvPnz3/55ZcSiSQmJsbU57WA43hiYuK8efOio6Pd3d0BADKZjLg8HaOQ6mG7VNZeRGRzqUIvpl5nJGLjd+/ezcjIAACIRKKpU6euW7dOJpNVV1e3Xken06lUKldXV9NbrVablpZGRJiuoFEaXX2YZLVuFnsREQDA5lFLHyqI2HJubu6GDRuSkpLEYnFeXl5CQoJIJPLw8GAyma6urpmZmdnZ2RQKxc/P79y5cxUVFc3NzV999VVISIhUKlUozETy8/MDAKSkpOTl5RERuPiezK0PXBfJ2pGI/sHcp3mEiLh48eLo6OgdO3a89dZbK1as4HK5cXFxNBoNALB8+fKsrKx169apVKqtW7eyWKzZs2fPmDFj5MiRa9euZbFYkyZNqqqqarNBb2/vqKio/fv3x8bGEhG4rEDpP9jaY/sdY0dXaGs1xosHq6NXe5EdhGSeFSlLH8rHz3YlO8j/YEc9IoNJcfVm3vuNwFNnNkHGuYbBo53ITtEWuA6diCZ8qmDv+pL27hw1Go0TJ040u0ir1dLpdAwzM+TRt2/fQ4cOWTrpC3JycmJiYrobKSAgIC4uzuy3iu/JXNwYIi+4jlTsa9dsIjet2WjEh48372J7QyoajYbJNP/HwzCMxyNwToUeRKJQKFyu+RLw4sGqN6NFjny6RTNaALsTEQBw6VD1wDAH25qRwyLA/MPtqEZsYcpyj9sXGuueq8kOYlVSE+sFHgw4LbTTHvHFeY7vK15/V2DrM910kdTEeldf5qARjmQHaRd77BFNhd3sGJ+sq+L8TOgumrcsOI6f3VfpyKfBbKH99ogt3L7Y8DRfGT5V4BcE1wCvRchOacrPlE6Y6+o7EPaO395FBAA0VmkyLjQy2RSvAWz/wVyOg80PadVXaMoLFXevi4e+6Twqkk+hwHWhjVmQiC+oLFEVZcme5itc3Oh8NwbXicZ1pHGdqAYD2cm6AIbhsia9QmrAjXjxPTmLS+k/jDf0TWfYLjrsACRiW2rKVPWVWoVEr5DqKRRMKbOkiSqVqrS0dPDgwRbcJgCA50IDOOA6Uh1caJ792A4u0A0TdgoS0aqUlJRs3Ljx5MmTZAeBDpvpuhG9GyQiAgqQiAgoQCIioACJiIACJCICCpCICChAIiKgAImIgAIkIgIKkIgIKEAiIqAAiYiAAiQiAgqQiAgoQCIioACJiIACJCICCpCICChAIiKgAImIgAIkIgIKkIgIKEAiWhUMw1qecIFoDRLRquA4XldXR3YKGEEiIqAAiYiAAiQiAgqQiAgoQCIioACJiIACJCICCpCICChAIiKgAImIgAIkIgIKkIgIKEAiIqAAiYiAAiQiAgrQA3+swfz585VKJQBAq9U2NjZ6eHiYHkF/5coVsqPBAuoRrcH06dNramqqqqoaGhpwHK+qqqqqqnJwcCA7F0QgEa3B/PnzfX19W3+CYdiYMWPISwQdSERrgGHYzJkzqVRqyyd9+vSZN28eqaHgAoloJebOnevj42N6jWHYuHHjTJUiwgQS0UrQaLT58+czmUwAgLe39+zZs8lOBBdIROsxc+ZMb29vAEB4eDjqDttAIzsAdBiNeHO9TtqgMxIwrhUV8X6KMWX8yHmleQqLb5xOx/geDK6jTf5N0Tji/1B0V5aXLlHKDZ7+HIVUT3ac7sF2oD4rVLj1YY2fLeI525iOSMQ/eZQtLbqrGD/XnULByM7Sc8R1mrRTNdFrvLhOtuQiqhFfUPJAXnhHPnG+h01bCABwcWVOXel7+OsysoN0DyTiCx7cbH5jei+ZlYZKw0ZGiu5caSQ7SDdAIgIAgFppqK/Qsnm2tC/rGJ4zrfqphuwU3QCJCAAA0kadex822SksiYOAYTTYUvWPRDSBKWQ2dozcMbgBKCS29IuQiAgoQCIioACJiIACJCICCpCICChAIiKgAImIgAIkIgIKkIgIKEAiIqAAiYiAAiQiAgqQiDbAmeST27ZvJjsFsSARbYCiogKyIxBO77kU1MrI5fJTp3+5k3W7rKxEwBeGh49bvmwVi8UCABiNxu93b7+VfoNBZ0REvBM8eNjGz2MST13h8wV6vf7goR8y/7hVV1cTHBwSPX3u66+/mHhkxsxJy5b+TSJpPnwkjs1mjwgbvXbNeoFAGPPJitzcewCAq1cvnj97g8fjkf3TCQH1iD0k6UzCseM/z5v7l61bdq1c+dGN1JTDR+JMi06dPnr+QtIHaz/dv/8XNptz8NAPAAAKhQIA2B3779OJx6JnzDt29Py4sRGb/7UhNe266Vt0Ov3EiSMUCiX5zPXD/018mJfz8+EfAQC7/hM3aFDw5Mnv/n49u7daiHrEnjN3zuJxYyP69PE3vc3Ly72TlbFyxYcAgCtXL4x9c+L4cZMAAIsWLruTlWFaR6PRXLl6YeGCpdOiZgEApkROz8vLPRJ/YNzYCNMKXl4+ixctBwAAnsOIsNHFxYWk/Tyrg0TsIXQ6PSv79jfbNz8pKdbr9QAAFxc+AMBgMJSVlUa+M61lzbFvRjx4cB8AUFxcqNVqR4SNblkUMiz08q/nJFKJk6MTACAgYFDLIgcHR4VCbvWfRRpIxB4SdyD20qXklSs/GhE22s3N/aeDey9dPgsAkCvkOI5zONyWNZ2cnE0v5HIZAOCDj/5fm02JmxpNImKYbd/J+iogEXsCjuPnLyTOnrVw6rvRpk9MkgEAOGwOAECn07WsLBa/uK1TIBQBANZ98rmXl0/rrbm6ulsxO6QgEXuCwWBQqVRC4Yv7oLVabcbtNNNrOp3u6upWVlbSsnJ6RqrphbeXr2k2sOEhYaZPxOImHMc5HI7VfwF0oKPmnkCj0Xx9/S7/eq6yqkIiaf73jq+GBIfIZFKFQgEACB899mrKxazsTBzHT50+KpNJTd/icDhLl6w8En/g4cMcrVabmnZ9/YbVu77/ptPmvLx8Cgvz7t3P0mq1xP84ckAi9pBNn29lMVlLl81e/N6M0NdGvv/+WhaTFT1rUnVN1ZL3VgwZMnzD39f+5b3o8vKns2ctBADQaHQAwPx57326/otjCT9HTR///e7tnh7e69b9s9O2ot6diWHYpxvWKJWWn0MMEtAkTAAAUPdccz2hbuoKny6s2zlqtbqursbX18/0NuHEkaNHD50/d8MiG+8ikgbdjRNViz/rY81GXwXUI1qehBNHVvxtUWJSgkTS/NvvV0+e+mXaNDQ/bCeggxXLs3TJColEfPXqhQM/xYpEbtEz5i1auIzsULCDRCSEjz78O9kRbAy0a0ZAARIRAQVIRAQUIBERUIBEREABEhEBBUhEBBQgERFQgEREQAESEQEFSEQAAKBQMUd+rzrbiRtxvjuT7BTdAIkIAABCT0ZZgcJIxPNISaKxWk1j2NIdMEjEFwSOcKx+qiQ7hcVoqtH4B9vSHQhIxBdMnCe6lVSrktvSQ3La4/7vjbgBHxDiQHaQboCu0AYAgKKiIqlUOmxIaPyW8mHj+TxnurMrAzeSHaubGI14Q6W6sUoNjPjE+Tb2gEskInjy5MkXX3xx6NAh08w12deaKh6rAI5J6i1/p5IRx3U6HZPBsPiWAQB8T+ajorwGVb7PIJqfn5+fn19gYCCNZhsHYXYtYkVFhbe3d0lJSb9+/azTYklJycaNG0+ePEnQ9jdu3HjlyhUMw1xcXHg8HpPJ9PT0DAgIWLVqFUEtWgr7FfHWrVvffvvt2bNnrdmoTCa7e/fu+PHjCdr+o0ePYmJiGhoaWn9oNBo9PDwuXrxIUKMWwR4PVuRyuckJK1sIAHBwcCDOQgBAYGDgoEGD2nzI5XIht9AeRTx37ty2bdsAAJGRkdZvvb6+/ocffiC0iYULF7q4uLS8pVAoN2/eJLRFi2BHIpqKkKKioi1btpCVQSqV3rhB7A3OI0aM6Nevn+nHGo3Gvn37Wr/j7wH2ImJKSkpycjIA4NNPPyUxhqur6+rVq4luZe7cuU5OTgAAHx+fhISE3NzcrVu3Et3oK2IXByulpaVxcXHffNP5LDO9hkWLFtXW1l67ds30NjEx8cyZM7/88gvZudoH79XcunWroaGhqamJ7CAvqKur27t3LylNFxQUhIaG5uXlkdJ6p/TmXfP169dPnDghEAhaF+/kYoUasT0GDRqUnZ29ffv206dPkxKgY3rnrrm4uDggIODhw4dDhgwhO8v/QPQ4YlfYtm2bVqvdvBmuB7f0QhEPHz5cXl7+xRdfkB0EXs6dO3f06NH4+HgGMScbewLZtYElMdWCZ8+eJTtIu5BYI7bh8ePHr7/++v3798kO8oLeUyMeOHDAdJA4bdq0LqxODiTWiG3o37//7du3Y2Njjx07RnYW0EvGEXU6XVVVlcFgmDNnDtlZOsE644hd5+DBg9XV1f/8Z+ez1hKNzdeIx44dGzlypK+vL0Tljq1x+fLlAwcOxMfHc7ncLqxOCLbdI6akpFRXV/fv399WLLTCueYeEBkZuXPnzsjIyKysLLIy2KqIV69eBQAMGTJk3bp1ZGfpBvDUiG3o06dPWlrawYMHDx8+TEoAmxRxz549Dx8+BAC4u9vYo3JgqxHbsH//folEsmHDBhLaJvuwvXsUFhbiOJ6bm0t2kN7MtWvXpk6dKhaLrdmoLfWImzZtKigoAAAMHTqU7Cw9BM4asQ0RERE//vjjrFmz0tPTrdaobYgoFotVKtXo0aNnzpxJdpZXAtoasQ2enp6mM/U//fSTdVq0ARG3bdtWWVnJZrOnTJlCdpZXBfIasQ27d+/W6XQff/yxFdqCfRwxNTW1vr5+9mz0wBzSSEtL27JlS3x8vKsrkfdKW7Mg7RaxsbE4jqtUKrKDWBJ4zjV3i/r6+nfeeScnJ4e4JiDdNSclJTU1NQEATDe99xpYLNb9+/fJTtFthELh5cuX9+7dW1lZSVATkO6a1Wo1jUazlVkKuoVOp9Pr9RiG2dy/sbCwsKysLAwjZJIxSHtEFovVKy00PVmczWafOHGiurqa7Czd4NGjRwMHDiTIQnhF3LVrV1JSEtkpCGTJkiUxMTFkp+gGhYWFL9+6b0EgFVGr1ep0OrJTEMuJEycAAM+fPyc7SJcoKCgICgoibvuQivjxxx/PmjWL7BTWIDU19e7du2Sn6Bw77RHpdHpvrRHbsHjx4suXL5OdonMePXpkjyL2+hqxNaYLpDMzM8kO0i4FBQWEWgiviPZQI7ahoqLiypUrZKcwD9H7ZXifYP/xxx8TN1IAJ7Nnzz516hTZKcxTUFBA9B3ikPaI9lMjtsZ089fx48fJDtIWK/SIkIpoVzViGwQCAVSzghiNxsePHw8cOJDQViAV0Q5rxBYmT57s5+dHdoo/IXoE0QSkItrPOKJZwsLCAACQzJpihf0yvCLaZ43Yhujo6KNHj5Kdwr5FtOcasYXhw4dPmDCB7BT2vWu25xqxNZ6enqaukawAer3+6dOnAwYMILohSEW08xqxDfv374+Pj2/9yeTJk63TtHW6Q3hFRDVia9zc3ObNmyeXy1UqFQBgypQpjY2Nn332mRWatk6BCO+ZlV27dvn6+tr6zaMWhMFgMBiMMWPGODs719XVYRiWn5/f1NTE5/MJbbegoGDEiBGENmEC0h4R1YhmEQgENTU1ptdNTU1WeJKP1XpESO9Z0el0GIahvXNrZs2aVV5e3vLWaDSGh4fv2bOHuBa1Wu24ceNu375NXBMtQNojohqxDdHR0U+fPjUa/3yGNIVCKS8vLy0tJa5Rqx2pwCsiGkdsw5kzZ6Kjo/38/JydnU3dIQCgtraW0L2z1fbL8B6soBrxZTZt2gQAePDgwc2bN2/evNnY2CgRK1Ov35k5bRFBLRblPxs+fLhMrO/xFnAcOPK75BhcNeLEiRMlEklLJAzDcBx3d3e/dOkS2dHgIjul6cEtsRHT6zU4m7D7o/V6PZVGe5XLQl08mJWPlf2HcUdNETjy6R2sCVePGB4efunSJQrlz4KBQqFERUWRGgo6fj1cw+PTI5f78pw7+tNCgl5nbK7Tnvq+YuYaLxfXdmeYhqtGXLBggemkVgve3t4LFiwgLxF0XP65xsWdOWyswCYsBADQ6BShF2vuJ/5n9lZKm9ott+AScfDgwcHBwS1vMQx75513TOU5AgBQVqBgsKlBr8PyaMFuMWGeR+alpvaWwiUiAOC9994TCoWm197e3nPnziU7EUTUPdfQmdD9ybqIixvzSY6svaXQ/aqgoKCWmYkjIyPhebAoDGiUBqEHk+wUPYRKw3wHcpvrtWaXQiciAGDp0qUCgcDd3R11h21QSA16Wx7UaqrVtndz5qseNVeVKCUNeoVMr5QajAag1xu78KVOEYwZuIrL5WZf1gBQ++qbY7IpGMA4jlSOI1XgyRR52mqn0ovpoYjlhYrie/LSPIWLOxvHMSqdSqFTKVSqpUYlg4eOBwDIFBbZGJArMaPBYKjUG7RqnVqiUxv6DeUGhjm49bGxGQp7Md0WsfqpKu1MI53DwGjMfqNdaHQqMcEIRKvSNzYoUpPFbA54c4bAWWQbj0/r3XRPxGvH66tK1QJ/PtfFhvsSBpvG93ECAEjrFImxVYNGOoRPFZAdyt7p6sGKXmf8+atytYHp+5qnTVvYGkdXbr/RPnU1lDN7iZoaGtFFuiSiQY/HbSz1CHLjCUh7jCpxOHs50p0cE3bYxoSZvZXORTQa8X0bSoIi/Jlc2zin1AN4Ao6jF//w/5V3YV0EIXQu4tFtzwaEe1klDJlwnFl8H+eLB21pgvXeRCci3khscPZxZnLt4rjSwZWnA8yc1Gayg9gjHYnYWKV5mqdwEPGsmIdknD2dbiU3QHWNpp3QkYhpyY1Cf2LvVoQQ9wCXm8mNZKewO9oVsaZMpTdQHEQc6+bpKjkPr63fNEquEFt8y0I/58pSjUZlsPiWbZQZMycdiSf8YbntivgkV4FRe+1hcidglLJ8JdkhLMO/vvrHpctnyU7ROe2KWPJA4eAKaXdINBw+93GOnOwUlqGoqIDsCF3C/Ck+cZ2W7UAn7mC57NmDq7//9LyigMd1GTRwzOQJ77NYXABAeuaplNRDq5bvO5Kwsbau1MOt/9jwBSNem2r61oVfY7NzLzEZnOFD33YV+hKUDQDg6MqpzpcSt32rMSEiDADw7Y6v9+3fef7sDQBAenrq4SNx5c+eOjk59+8/8KMP/u7m5m5auYNFLWT+kX7ixJFHRfl8vjA4eNiK9z8QCIQWiWq+R5Q369Uqi1zQZYaGxuc//vyBTqdZu+KnJQu3V9c+3ndolcGgBwBQaXSVSpZ8ccfcGZ99+1Xm0OCJJ5P/T9xcAwDIuJOYcef0zHc//WjlfwUunim/HyQonukWBblYp5D2/DZKSPj1UjoA4NP1m0wWZt/944svP508+d2TCZc2b/qmtrZ61+5vTGt2sKiF4sePNn720fDhI34+dPrDDzaUlBRv//eXlopqXkSl1EAl7LKae7m/0qj0pQu2u4n83F37zpn+eWV1UV5hqmmpwaB7a8L7fXyGYBgWFvIujuOV1cUAgFu3Tw4dHDE0eCKH4zjitan9+4YRFM8Eg0VVSGxexDYc+u++sW9OnD1roZOT8+DBQ1ev+iQz89ajooKOF7WQ9zCHxWItXrTczc191Mjw777dt2DBUktla0dEmZ7KIOpO07JnD3y8g7jcF7dE8V08BHzvp+U5LSv4eg02veCwHQEAKrUMx/GGpudurv4t63h7BhIUzwSdTVXafo/YhtLSx4GBg1veDgwIAgA8epTf8aIWgoeEqNXqjZ/HnDp9tKLyuZOT8/AQi3UH7dqGAaIGdVVq+fPKgvWbRrX+UCr7c+ju5avJ1RqF0WhgMv88eGIw2ATFM2E0ANC7njgkl8s1Gg2T+eeVUxwOBwCgVCo6WNR6CwEDAr/Ztjst7Xrcgdgf9u0MfW3k0iUrg4OHWSSeeRE5jjSDTm2RBl7GwUHg3yfk7YkrWn/I5Tp18BUWk0uhUHWtImm0xA6vGLQGriNcsw+8IiwWCwCgVqtaPlEoFQAAAV/YwaI2Gxk1MnzUyPBlS/929+4fiUnHP/s85kzSNSrVAlWc+V0zx4Fq0BE1ouvpNqBZUtPXb3j/vqGm/3g8F1dhR08WwTDMxdmj7NnDlk8Ki9IJimdCqzZwHG3v4vMOoNFoAwMG5ec/aPnE9LpvvwEdLGq9hZycu3/cyQAACIWit9+eumb1Oplc1tBQb5F45kV05NPoDKJ2TGPDFxiNxnOXd2q16rr68gtX9ny3Z2F17ZOOvzUseNLDgt9zHl4DAPx280h5RR5B8UxXvvGcab2gR2QymSKRa3Z25v2cbL1eHz1j3q30G4mJx6Uy6f2c7B/2/ee14SMG9B8IAOhgUQt5+blf/mvD+QtJzc3igsK8pDMJQqFIKBRZJKr5/9dOQoZebVDLtCwHyw8lcjiO69ce+/1m/K79S+rqy3y9B8+Z8XmnBx+Txi1TKMTJl7775eTn/n1CpkXGHDv1BUFXJ0hrFS6uveSs0qKFy//78/47WRnHj12YPPnd+oa6E6fi9/zwnZube1jo6399f61ptQ4WtTB3zuLmZvGevTv+s3Mrg8GYOOHtnf+Js8h+uaPZwG5fbKwow0V97fH+9qr8uhERvAHDHcgO0pZfD9d49uP5D7HV66HOxJZP/5unk9DMP/J2T/H1H8bF9b1t/KKLYJjBf3AvvCkCZtotg0TeLDYHl9QqnNzM/0maJXU79pifp4vN5Kk05s/Vuov6rl1xoKdpzfDPLRHtLTIY9FSqmR/o6z14xZLd7X2rvlTsH8SmMWCcA6MX01E9Pnam8PSuyvZEdODxP1kdb3aRVqtmMMzf6UehWPgIoL0MAACtTsOgm5nUgUZrt/A1Goz1TyVz1vSzXEBEl+hICycBfdAoXmO9zEFkplqiUml8F09z37Mqls0grZaMn2OZs/iIbtHJDih8qlDZIFc2EzW4DRWSaimPawwa1dHQOoIgOq+E5n3i/ex+jU7dyw9cmmvkqib5pIWuZAexU7pUkq/c3vdx+vNe3C9KauRArZi/3ofsIPZLl0TEMGz1jv7SyiZpbbszftou4udiBqaasYr8etee6cYgxfz1PgKBoTSzQlpnoeniyEZcKX10o9x/IC1yadtLkRFWpnuDKW9ECYJGOaSdaWwoUeJUuqOIa4vzkKikGlm90qjRCD3pU77sw2T3qosbbJRuj+q5uDKmr/SoKVM/zpGXPKhlcmhGI0ZlUKl0KoVGBYRdxfgqYBim1xmMWr1ea9CqdEw2ZUAIL+A1EZoZER56OLzs7sdy92O9OUPYVKOVNOgUUr1CojfojQY9jCIyWBiFSuE6cjiOVKEXg+dke714r+dVz3Pw3Rl8d9SvIF4VdEbVluA60Wx60gO+O7O94g2JaEuwuZSGSg3ZKXqITmusKFY4Cc3vP5GItoRbH5ZOY6uT8jTVaDq4xBOJaEv4BHAwDNz/zSYnK/vtWNUb09qdNB+u5zUjukJaUr1Oh/cb6ijwtIFZ9RW6zPHgAAAAZ0lEQVRSvaRe83tCzV8+9+W2P16BRLRJ8m5L8jOkaqVBQ9jMMBZB5MVsrtP6D+G+ESXs+HGWSEQbBseBVg21iLgRZ3G7dOIKiYiAAnSwgoACJCICCpCICChAIiKgAImIgAIkIgIK/j88u/2J087bqAAAAABJRU5ErkJggg==",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Initialize tools and reasoner\n",
        "regex_tool = RegexExtractionTool()\n",
        "validation_tool = ValidationTool()\n",
        "duplicates_tool = RemoveDuplicates()\n",
        "processing_tool = CreateJSON()\n",
        "tools = [regex_tool, validation_tool, duplicates_tool, processing_tool]\n",
        "\n",
        "reasoner = UnitReadingReasoner(tools)\n",
        "\n",
        "# Create the workflow\n",
        "workflow = StateGraph(WorkflowState)\n",
        "\n",
        "def agent_node(state: WorkflowState):\n",
        "    \"\"\"Main agent node using the UnitReadingReasoner class.\"\"\"\n",
        "    messages = state[\"messages\"]\n",
        "    last_message = messages[-1]\n",
        "\n",
        "    if isinstance(last_message, HumanMessage):\n",
        "        # Initial reasoning\n",
        "        response_text, tool_calls = reasoner.reason(last_message.content)\n",
        "\n",
        "        if tool_calls:\n",
        "            # Create AI message indicating tool usage\n",
        "            ai_message = AIMessage(\n",
        "                content=response_text,\n",
        "                additional_kwargs={\"tool_calls\": tool_calls}\n",
        "            )\n",
        "            return {\n",
        "                \"messages\": [ai_message],\n",
        "                \"pending_tool_calls\": tool_calls,\n",
        "                \"current_step\": 1,\n",
        "                \"all_results\": []\n",
        "            }\n",
        "        else:\n",
        "            # Direct response without tools\n",
        "            return {\"messages\": [AIMessage(content=response_text)]}\n",
        "\n",
        "    # Handle completion of processing steps\n",
        "    elif state.get(\"all_results\"):\n",
        "        # Check if we need to continue processing\n",
        "        current_step = state.get(\"current_step\", 0)\n",
        "\n",
        "        if current_step > 0 and current_step < 4:  # Still have steps to process\n",
        "            # Get the last result to determine next step\n",
        "            last_result = state[\"all_results\"][-1] if state[\"all_results\"] else None\n",
        "\n",
        "            if current_step == 1:  # After extraction\n",
        "                extraction_matches = last_result['output'] if last_result and 'output' in last_result else []\n",
        "                next_step = reasoner.get_next_step(current_step)\n",
        "                if next_step:\n",
        "                    next_step['args']['matches'] = extraction_matches\n",
        "                    return {\n",
        "                        \"pending_tool_calls\": [next_step],\n",
        "                        \"current_step\": current_step + 1\n",
        "                    }\n",
        "\n",
        "            elif current_step == 2:  # After validation\n",
        "                # Get the original dictionary output from the validation tool\n",
        "                validation_result_dict = last_result['output'] if last_result and 'output' in last_result and isinstance(last_result['output'], dict) else {}\n",
        "                next_step = reasoner.get_next_step(current_step, validation_result_dict)\n",
        "\n",
        "                if next_step:\n",
        "                    # Get original extraction results to pass to processing\n",
        "                    extraction_result = next((r for r in state[\"all_results\"] if r.get('step') == 1), None)\n",
        "                    extraction_matches = extraction_result['output'] if extraction_result and 'output' in extraction_result else []\n",
        "                    next_step['args']['matches'] = extraction_matches\n",
        "                    return {\n",
        "                        \"pending_tool_calls\": [next_step],\n",
        "                        \"current_step\": current_step + 1\n",
        "                    }\n",
        "                else:\n",
        "                    # Validation failed, provide final response\n",
        "                    user_input = next(msg.content for msg in messages if isinstance(msg, HumanMessage))\n",
        "                    final_response = reasoner.synthesize_results(user_input, state[\"all_results\"])\n",
        "                    return {\"messages\": [AIMessage(content=final_response)]}\n",
        "\n",
        "            elif current_step == 3:  # After removing duplicates\n",
        "                # Get the unique matches from the last result  \n",
        "                unique_matches = last_result['output'] if last_result and 'output' in last_result else []\n",
        "                next_step = reasoner.get_next_step(current_step)\n",
        "                if next_step:\n",
        "                    next_step['args']['matches'] = unique_matches\n",
        "                    return {\n",
        "                        \"pending_tool_calls\": [next_step],\n",
        "                        \"current_step\": current_step + 1\n",
        "                    }\n",
        "\n",
        "        # Processing complete, synthesize final response\n",
        "        user_input = next(msg.content for msg in messages if isinstance(msg, HumanMessage))\n",
        "        final_response = reasoner.synthesize_results(user_input, state[\"all_results\"])\n",
        "        return {\"messages\": [AIMessage(content=final_response)]}\n",
        "\n",
        "    return {\"messages\": [AIMessage(content=\"Processing complete.\")]}\n",
        "\n",
        "def tool_node(state: WorkflowState):\n",
        "    \"\"\"Execute pending tool calls.\"\"\"\n",
        "    pending_calls = state.get(\"pending_tool_calls\", [])\n",
        "    tool_results = []\n",
        "    tool_messages = []\n",
        "    current_step = state.get(\"current_step\", 1)\n",
        "\n",
        "    for call in pending_calls:\n",
        "        tool_name = call[\"tool\"]\n",
        "        args = call[\"args\"]\n",
        "        step = call.get(\"step\", current_step)\n",
        "\n",
        "        result = reasoner.execute_tool_call(tool_name, args)\n",
        "        tool_result = {\n",
        "            \"tool\": tool_name,\n",
        "            \"args\": args,\n",
        "            \"output\": result,\n",
        "            \"step\": step\n",
        "        }\n",
        "        tool_results.append(tool_result)\n",
        "\n",
        "        # Create tool message (convert result to string for message content)\n",
        "        tool_message_content = json.dumps(result) if isinstance(result, (dict, list)) else str(result)\n",
        "        tool_message = ToolMessage(\n",
        "            content=f\"Step {step} ({tool_name}): {tool_message_content}\",\n",
        "            tool_call_id=f\"{tool_name}_{step}_{hash(str(args))}\"\n",
        "        )\n",
        "        tool_messages.append(tool_message)\n",
        "\n",
        "    # Update all_results with new results\n",
        "    all_results = state.get(\"all_results\", [])\n",
        "    all_results.extend(tool_results)\n",
        "\n",
        "    return {\n",
        "        \"messages\": tool_messages,\n",
        "        \"pending_tool_calls\": [],\n",
        "        \"all_results\": all_results\n",
        "    }\n",
        "\n",
        "def should_continue(state: WorkflowState) -> str:\n",
        "    \"\"\"Determine whether to continue with tools or end.\"\"\"\n",
        "    if state.get(\"pending_tool_calls\"):\n",
        "        return \"tools\"\n",
        "\n",
        "    # Check if we need more processing steps\n",
        "    current_step = state.get(\"current_step\", 0)\n",
        "    if current_step > 0 and current_step < 4:\n",
        "        # Check if the last step was successful and requires a follow-up\n",
        "        if state.get(\"all_results\"):\n",
        "            last_result = state[\"all_results\"][-1]\n",
        "            if last_result.get('step') == 1: # After extraction, always go to validation\n",
        "                 return \"agent\"\n",
        "            elif last_result.get('step') == 2: # After validation, check if valid for processing\n",
        "                 validation_output = last_result.get('output')\n",
        "                 if isinstance(validation_output, dict) and validation_output.get('is_valid', False): # Validated successfully\n",
        "                     return \"agent\"\n",
        "                #  elif isinstance(validation_output, dict) and validation_output.get('duplicates_found', False): # Duplicates found, go to remove duplicates\n",
        "                #      return \"agent\"\n",
        "                 else:\n",
        "                     return END # Validation failed, end workflow\n",
        "            # elif last_result.get('step') == 3: # After removing duplicates, always go to processing\n",
        "            #      return \"agent\"\n",
        "            else:\n",
        "                 return END # unexpected step\n",
        "\n",
        "    return END\n",
        "\n",
        "# Add nodes and edges\n",
        "workflow.add_node(\"agent\", agent_node)\n",
        "workflow.add_node(\"tools\", tool_node)\n",
        "\n",
        "workflow.add_edge(START, \"agent\")\n",
        "workflow.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    should_continue,\n",
        "    {\n",
        "        \"tools\": \"tools\",\n",
        "        END: END,\n",
        "    }\n",
        ")\n",
        "workflow.add_edge(\"tools\", \"agent\")\n",
        "\n",
        "\n",
        "# Compile the workflow\n",
        "app = workflow.compile()\n",
        "display(Image(app.get_graph(xray=True).draw_mermaid_png()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Example Usage**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test function\n",
        "def run_workflow(user_input: str):\n",
        "    \"\"\"Run the workflow with user input.\"\"\"\n",
        "    initial_state = {\n",
        "        \"messages\": [HumanMessage(content=user_input)],\n",
        "        \"current_step\": 0,\n",
        "        \"extraction_results\": [],\n",
        "        \"validation_results\": {},\n",
        "        \"processing_results\": [],\n",
        "        \"pending_tool_calls\": [],\n",
        "        \"all_results\": []\n",
        "    }\n",
        "\n",
        "    result = app.invoke(initial_state)\n",
        "    \n",
        "    print(f\"\\nUser: {user_input}\")\n",
        "    for message in result[\"messages\"]:\n",
        "        # if isinstance(message, AIMessage):\n",
        "        #     print(f\"Assistant: {message.content}\")\n",
        "        # elif isinstance(message, ToolMessage):\n",
        "        #     print(f\"Tool: {message.content}\")\n",
        "        print(f\"{message.__class__.__name__}: {message.content}\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "test_inputs = [\n",
        "        \"Unit 1A reads 123.45 cubic meter and Unit 2B reads 67.89 cubic meter\",\n",
        "        \"Unit 1A is 100.0 cubic meter, Unit 2B reading 150.5 cubic meter, Unit 3C reads 200.5 m3\", #Mixed case\n",
        "        \"Unit 1A reads 123.45 cubic meter and Unit 1A reads 123.45 cubic meter\",  # Duplicate\n",
        "        \"Unit 1A reads 123.45 cubic meter and Unit 1A reads 200.0 cubic meter\",   # Conflict\n",
        "        \"This is just regular text with no unit readings\",\n",
        "        \"Unit 1A reads 50.0 cubic meter, Unit 2B reads 75.5 cubic meter, Unit 1A reads 50.0 cubic meter, Unit 3C reads 100.25 cubic meter\"  # Mixed case\n",
        "    ]\n",
        "\n",
        "# for test_input in test_inputs:\n",
        "\n",
        "# for test in test_inputs:\n",
        "#     run_workflow(test)  # Example input to test the workflow\n",
        "# run_workflow(\"13A reads 24 cubic meter, 13A reads 24 m3\") # Example input to test the workflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Test Case 1 (No Duplicates/Conflicts)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            " ReAct Simulation\n",
            "================================================================================\n",
            "💭 Thought: I need to check if there is a viable input\n",
            "🔧 Action: Checking for inputs using _contains_unit_readings()\n",
            "👁️ Observation: Viable input found\n",
            "💭 Thought: I'll process your unit readings step by step.\n",
            "🔧 Action: Executing tool 'regex_extraction' with args: {'input_text': 'Unit 12A reads 321 cubic meter and Unit 2B reads 67.89 cubic meter'}\n",
            "👁️ Observation: Unit and Water Meter Reading pairs found\n",
            "💭 Thought: I'll validate the extracted unit readings\n",
            "🔧 Action: Executing tool 'validation' with args: {'matches': [('12A', '321'), ('2B', '67.89')]}\n",
            "👁️ Observation: Validation done, no issues found\n",
            "💭 Thought: I'll process the validated matches into JSON file\n",
            "🔧 Action: Executing tool 'data_processing' with args: {'matches': [('12A', '321'), ('2B', '67.89')]}\n",
            "👁️ Observation: Final JSON is ready to export\n",
            "================================================================================\n",
            "\n",
            "\n",
            "User: Unit 12A reads 321 cubic meter and Unit 2B reads 67.89 cubic meter\n",
            "HumanMessage: Unit 12A reads 321 cubic meter and Unit 2B reads 67.89 cubic meter\n",
            "AIMessage: I'll process your unit readings step by step.\n",
            "ToolMessage: Step 1 (regex_extraction): [[\"12A\", \"321\"], [\"2B\", \"67.89\"]]\n",
            "ToolMessage: Step 2 (validation): {\"is_valid\": true, \"errors\": [], \"duplicates_found\": false, \"conflicts_found\": false, \"conflicts\": {}}\n",
            "ToolMessage: Step 4 (data_processing): [{\"unit\": \"12A\", \"reading\": 321}, {\"unit\": \"2B\", \"reading\": 67}]\n",
            "AIMessage: 📊 **Extraction Results:**\n",
            "Found 2 unit readings:\n",
            "  - Unit 12A: 321 cubic meters\n",
            "  - Unit 2B: 67.89 cubic meters\n",
            "\n",
            "✅ **Validation Results:**\n",
            "All unit readings are valid!\n",
            "\n",
            "🔧 **Final Processed Data:**\n",
            "```json\n",
            "[\n",
            "  {\n",
            "    \"unit\": \"12A\",\n",
            "    \"reading\": 321\n",
            "  },\n",
            "  {\n",
            "    \"unit\": \"2B\",\n",
            "    \"reading\": 67\n",
            "  }\n",
            "]\n",
            "```\n",
            "\n",
            "📈 **Summary:**\n",
            "Successfully processed 2 unique unit readings\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "run_workflow(\"Unit 12A reads 321 cubic meter and Unit 2B reads 67.89 cubic meter\") "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Test Case 2 (With Duplicates)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            " ReAct Simulation\n",
            "================================================================================\n",
            "💭 Thought: I need to check if there is a viable input\n",
            "🔧 Action: Checking for inputs using _contains_unit_readings()\n",
            "👁️ Observation: Viable input found\n",
            "💭 Thought: I'll process your unit readings step by step.\n",
            "🔧 Action: Executing tool 'regex_extraction' with args: {'input_text': 'Unit 2B reads 67.89 cubic meter and Unit 2B reads 67.89 cubic meter'}\n",
            "👁️ Observation: Unit and Water Meter Reading pairs found\n",
            "💭 Thought: I'll validate the extracted unit readings\n",
            "🔧 Action: Executing tool 'validation' with args: {'matches': [('2B', '67.89'), ('2B', '67.89')]}\n",
            "👁️ Observation: Duplicates found, Duplicate unit found: 2B\n",
            "💭 Thought: I'll remove extraction matches duplicates\n",
            "🔧 Action: Executing tool 'remove_duplicates' with args: {'matches': [('2B', '67.89'), ('2B', '67.89')]}\n",
            "👁️ Observation: Final JSON is ready to export\n",
            "================================================================================\n",
            "\n",
            "\n",
            "User: Unit 2B reads 67.89 cubic meter and Unit 2B reads 67.89 cubic meter\n",
            "HumanMessage: Unit 2B reads 67.89 cubic meter and Unit 2B reads 67.89 cubic meter\n",
            "AIMessage: I'll process your unit readings step by step.\n",
            "ToolMessage: Step 1 (regex_extraction): [[\"2B\", \"67.89\"], [\"2B\", \"67.89\"]]\n",
            "ToolMessage: Step 2 (validation): {\"is_valid\": true, \"errors\": [\"Duplicate unit found: 2B\"], \"duplicates_found\": true, \"conflicts_found\": false, \"conflicts\": {}}\n",
            "ToolMessage: Step 3 (remove_duplicates): [[\"2B\", \"67.89\"]]\n",
            "AIMessage: 📊 **Extraction Results:**\n",
            "Found 2 unit readings:\n",
            "  - Unit 2B: 67.89 cubic meters\n",
            "  - Unit 2B: 67.89 cubic meters\n",
            "\n",
            "✅ **Validation Results:**\n",
            "All unit readings are valid!\n",
            "  🔄 Duplicates detected\n",
            "\n",
            "🔄 **Duplicates Removed:**\n",
            "Found 1 unique unit readings:\n",
            "  - Unit 2B: 67.89 cubic meters\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "run_workflow(\"Unit 2B reads 67.89 cubic meter and Unit 2B reads 67.89 cubic meter\") "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Test Case 3 (With Conflics)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            " ReAct Simulation\n",
            "================================================================================\n",
            "💭 Thought: I need to check if there is a viable input\n",
            "🔧 Action: Checking for inputs using _contains_unit_readings()\n",
            "👁️ Observation: Viable input found\n",
            "💭 Thought: I'll process your unit readings step by step.\n",
            "🔧 Action: Executing tool 'regex_extraction' with args: {'input_text': 'Unit 2B reads 63.89 cubic meter and Unit 2B reads 67.89 cubic meter'}\n",
            "👁️ Observation: Unit and Water Meter Reading pairs found\n",
            "💭 Thought: I'll validate the extracted unit readings\n",
            "🔧 Action: Executing tool 'validation' with args: {'matches': [('2B', '63.89'), ('2B', '67.89')]}\n",
            "👁️ Observation: Conflicting values found, Unit 2B: 67.89, 63.89\n",
            "================================================================================\n",
            "\n",
            "\n",
            "User: Unit 2B reads 63.89 cubic meter and Unit 2B reads 67.89 cubic meter\n",
            "HumanMessage: Unit 2B reads 63.89 cubic meter and Unit 2B reads 67.89 cubic meter\n",
            "AIMessage: I'll process your unit readings step by step.\n",
            "ToolMessage: Step 1 (regex_extraction): [[\"2B\", \"63.89\"], [\"2B\", \"67.89\"]]\n",
            "ToolMessage: Step 2 (validation): {\"is_valid\": false, \"errors\": [\"Duplicate unit found: 2B\", \"Conflicting readings for Unit 2B: 67.89, 63.89 cubic meters\"], \"duplicates_found\": true, \"conflicts_found\": true, \"conflicts\": {\"2B\": [\"67.89\", \"63.89\"]}}\n",
            "AIMessage: 📊 **Extraction Results:**\n",
            "Found 2 unit readings:\n",
            "  - Unit 2B: 63.89 cubic meters\n",
            "  - Unit 2B: 67.89 cubic meters\n",
            "\n",
            "✅ **Validation Results:**\n",
            "Validation issues found:\n",
            "  ⚠️ Duplicate unit found: 2B\n",
            "  ⚠️ Conflicting readings for Unit 2B: 67.89, 63.89 cubic meters\n",
            "  🔄 Duplicates detected\n",
            "  ⚡ Conflicts detected:\n",
            "    - Unit 2B: 67.89, 63.89\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "run_workflow(\"Unit 2B reads 63.89 cubic meter and Unit 2B reads 67.89 cubic meter\") "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Test Case 4 (Plain Text)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            " ReAct Simulation\n",
            "================================================================================\n",
            "💭 Thought: I need to check if there is a viable input\n",
            "🔧 Action: Checking for inputs using _contains_unit_readings()\n",
            "👁️ Observation: No viable input found\n",
            "\n",
            "User: This is just a plain text\n",
            "HumanMessage: This is just a plain text\n",
            "AIMessage: I don't detect any unit readings in your input. Please provide text containing unit readings in the format 'Unit 1A reads 123.45 cubic meter'.\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "run_workflow(\"This is just a plain text\") "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Test Case 5 (Plain Text with Keyword)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            " ReAct Simulation\n",
            "================================================================================\n",
            "💭 Thought: I need to check if there is a viable input\n",
            "🔧 Action: Checking for inputs using _contains_unit_readings()\n",
            "👁️ Observation: Viable input found\n",
            "💭 Thought: I'll process your unit readings step by step.\n",
            "🔧 Action: Executing tool 'regex_extraction' with args: {'input_text': 'This is just a plain text with cubic meter'}\n",
            "👁️ Observation: Unit and Water Meter Reading pairs found\n",
            "💭 Thought: I'll validate the extracted unit readings\n",
            "🔧 Action: Executing tool 'validation' with args: {'matches': []}\n",
            "================================================================================\n",
            "\n",
            "\n",
            "User: This is just a plain text with cubic meter\n",
            "HumanMessage: This is just a plain text with cubic meter\n",
            "AIMessage: I'll process your unit readings step by step.\n",
            "ToolMessage: Step 1 (regex_extraction): []\n",
            "ToolMessage: Step 2 (validation): {\"is_valid\": false, \"errors\": [\"No unit readings found\"], \"duplicates_found\": false, \"conflicts_found\": false, \"conflicts\": {}}\n",
            "AIMessage: 📊 **Extraction Results:**\n",
            "No unit readings found in the input.\n",
            "\n",
            "✅ **Validation Results:**\n",
            "Validation issues found:\n",
            "  ⚠️ No unit readings found\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "run_workflow(\"This is just a plain text with cubic meter\") "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
